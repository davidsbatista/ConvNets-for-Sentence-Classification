{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TREC - Question Answering (multi-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREC - Question Answering\n",
    "# http://cogcomp.cs.illinois.edu/Data/QA/QC/\n",
    "\n",
    "TREC_Question = namedtuple(\"TREC_Question\", \"label question\")\n",
    "\n",
    "trec_train = set()\n",
    "trec_test = set()\n",
    "\n",
    "for filename in os.listdir(\"TREC/\"):\n",
    "    with open(\"TREC/\"+filename,'r', encoding='latin_1') as f_input:\n",
    "        for line in f_input:\n",
    "            label, question = line.split(' ', 1)\n",
    "            label = label.split(':')[0]\n",
    "            question = TREC_Question(label, question.strip())\n",
    "            if filename=='TREC_10.label':\n",
    "                trec_test.add(question)\n",
    "            else:\n",
    "                trec_train.add(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 5381\n",
      "Test Samples : 500\n",
      "Labels       : {'ENTY', 'LOC', 'ABBR', 'NUM', 'HUM', 'DESC'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Samples: {}\".format(len(trec_train)))\n",
    "print(\"Test Samples : {}\".format(len(trec_test)))\n",
    "print(\"Labels       : {}\".format({x.label for x in trec_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8461 unique tokens.\n",
      "Max. sequence lenght:  33\n",
      "Shape of train data tensor: (5381, 33)\n",
      "Shape of train label tensor: (5381, 6)\n"
     ]
    }
   ],
   "source": [
    "# built two lists with sentences and labels\n",
    "questions_train = [x.question for x in trec_train]\n",
    "labels_train = [x.label for x in trec_train]\n",
    "\n",
    "# convert list of tokens/words to indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(questions_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(questions_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# get the max sentence lenght, needed for padding\n",
    "max_input_lenght = max([len(x) for x in sequences_train])\n",
    "print(\"Max. sequence lenght: \", max_input_lenght)\n",
    "\n",
    "# pad all the sequences of indexes to the 'max_input_lenght'\n",
    "data_train = pad_sequences(sequences_train, maxlen=max_input_lenght, padding='post', truncating='post')\n",
    "\n",
    "# Encode the labels, each must be a vector with dim = num. of possible labels\n",
    "le = LabelEncoder()\n",
    "le.fit(labels_train)\n",
    "labels_encoded_train = le.transform(labels_train)\n",
    "categorical_labels_train = to_categorical(labels_encoded_train, num_classes=None)\n",
    "print('Shape of train data tensor:', data_train.shape)\n",
    "print('Shape of train label tensor:', categorical_labels_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TREC: test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test data tensor: (500, 33)\n",
      "Shape of test labels tensor: (500, 6)\n"
     ]
    }
   ],
   "source": [
    "# pre-process test data\n",
    "questions_test = [x.question for x in trec_test]\n",
    "y_test = [x.label for x in trec_test]\n",
    "sequences_test = tokenizer.texts_to_sequences(questions_test)\n",
    "x_test = pad_sequences(sequences_test, maxlen=max_input_lenght)\n",
    "\n",
    "labels_encoded_test = le.transform(y_test)\n",
    "categorical_labels_test = to_categorical(labels_encoded_test, num_classes=None)\n",
    "print('Shape of test data tensor:', x_test.shape)\n",
    "print('Shape of test labels tensor:', categorical_labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convnets_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with random word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = get_cnn_rand(300, len(word_index)+1, max_input_lenght, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dsbatista/virtual_envs/python3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "5381/5381 [==============================] - 7s 1ms/step - loss: 0.4493 - acc: 0.8255\n",
      "Epoch 2/10\n",
      "5381/5381 [==============================] - 6s 1ms/step - loss: 0.3389 - acc: 0.8637\n",
      "Epoch 3/10\n",
      "5381/5381 [==============================] - 7s 1ms/step - loss: 0.2801 - acc: 0.8864\n",
      "Epoch 4/10\n",
      "5381/5381 [==============================] - 7s 1ms/step - loss: 0.2351 - acc: 0.9065\n",
      "Epoch 5/10\n",
      "5381/5381 [==============================] - 8s 1ms/step - loss: 0.2033 - acc: 0.9215\n",
      "Epoch 6/10\n",
      "5381/5381 [==============================] - 8s 1ms/step - loss: 0.1735 - acc: 0.9342\n",
      "Epoch 7/10\n",
      "5381/5381 [==============================] - 7s 1ms/step - loss: 0.1451 - acc: 0.9482\n",
      "Epoch 8/10\n",
      "5381/5381 [==============================] - 7s 1ms/step - loss: 0.1268 - acc: 0.9552\n",
      "Epoch 9/10\n",
      "5381/5381 [==============================] - 8s 1ms/step - loss: 0.0986 - acc: 0.9665\n",
      "Epoch 10/10\n",
      "5381/5381 [==============================] - 8s 1ms/step - loss: 0.0866 - acc: 0.9714\n"
     ]
    }
   ],
   "source": [
    "history = model_1.fit(x=data_train, y=categorical_labels_train, batch_size=50, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9016666593551635"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model_1.evaluate(x_test, categorical_labels_test, verbose=0)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       1.00      0.44      0.62         9\n",
      "        DESC       0.57      0.34      0.43       138\n",
      "        ENTY       0.37      0.68      0.48        94\n",
      "         HUM       0.93      0.86      0.90        65\n",
      "         LOC       0.90      0.85      0.87        81\n",
      "         NUM       0.97      0.89      0.93       113\n",
      "\n",
      "   micro avg       0.68      0.68      0.68       500\n",
      "   macro avg       0.79      0.68      0.70       500\n",
      "weighted avg       0.73      0.68      0.69       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = model_1.predict(x_test)\n",
    "class_predictions = [np.argmax(x) for x in raw_predictions]\n",
    "print(classification_report(y_test, le.inverse_transform(class_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with pre-trained static word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "Matrix shape: (8462, 100)\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_fasttext_embeddings()\n",
    "embeddings_matrix = create_embeddings_matrix(embeddings_index, word_index, 100)\n",
    "embedding_layer_static = get_embeddings_layer(embeddings_matrix, 'embedding_layer_static', max_input_lenght, trainable=False)\n",
    "model_2 = get_cnn_pre_trained_embeddings(embedding_layer_static, max_input_lenght, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5381/5381 [==============================] - 2s 303us/step - loss: 0.3628 - acc: 0.8528\n",
      "Epoch 2/10\n",
      "5381/5381 [==============================] - 1s 228us/step - loss: 0.2358 - acc: 0.9065\n",
      "Epoch 3/10\n",
      "5381/5381 [==============================] - 1s 211us/step - loss: 0.1858 - acc: 0.9280\n",
      "Epoch 4/10\n",
      "5381/5381 [==============================] - 1s 212us/step - loss: 0.1455 - acc: 0.9477\n",
      "Epoch 5/10\n",
      "5381/5381 [==============================] - 1s 209us/step - loss: 0.1133 - acc: 0.9652\n",
      "Epoch 6/10\n",
      "5381/5381 [==============================] - 1s 225us/step - loss: 0.0913 - acc: 0.9750\n",
      "Epoch 7/10\n",
      "5381/5381 [==============================] - 1s 220us/step - loss: 0.0704 - acc: 0.9843\n",
      "Epoch 8/10\n",
      "5381/5381 [==============================] - 1s 228us/step - loss: 0.0535 - acc: 0.9910\n",
      "Epoch 9/10\n",
      "5381/5381 [==============================] - 1s 251us/step - loss: 0.0409 - acc: 0.9953\n",
      "Epoch 10/10\n",
      "5381/5381 [==============================] - 1s 256us/step - loss: 0.0320 - acc: 0.9967\n"
     ]
    }
   ],
   "source": [
    "history = model_2.fit(x=data_train, y=categorical_labels_train, batch_size=50, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8816666488647461"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model_2.evaluate(x_test, categorical_labels_test, verbose=0)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       1.00      0.11      0.20         9\n",
      "        DESC       0.78      0.05      0.10       138\n",
      "        ENTY       0.31      0.74      0.44        94\n",
      "         HUM       0.96      0.69      0.80        65\n",
      "         LOC       0.55      0.68      0.61        81\n",
      "         NUM       0.70      0.73      0.72       113\n",
      "\n",
      "   micro avg       0.52      0.52      0.52       500\n",
      "   macro avg       0.72      0.50      0.48       500\n",
      "weighted avg       0.66      0.52      0.48       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = model_2.predict(x_test)\n",
    "class_predictions = [np.argmax(x) for x in raw_predictions]\n",
    "print(classification_report(y_test, le.inverse_transform(class_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNN with pre-trained dynamic word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer_dynamic = get_embeddings_layer(embeddings_matrix, 'embedding_layer_dynamic', max_input_lenght, trainable=True)\n",
    "model_3 = get_cnn_pre_trained_embeddings(embedding_layer_dynamic, max_input_lenght, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5381/5381 [==============================] - 3s 573us/step - loss: 0.3788 - acc: 0.8457\n",
      "Epoch 2/10\n",
      "5381/5381 [==============================] - 2s 464us/step - loss: 0.2246 - acc: 0.9111\n",
      "Epoch 3/10\n",
      "5381/5381 [==============================] - 2s 438us/step - loss: 0.1522 - acc: 0.9460\n",
      "Epoch 4/10\n",
      "5381/5381 [==============================] - 2s 457us/step - loss: 0.1053 - acc: 0.9663\n",
      "Epoch 5/10\n",
      "5381/5381 [==============================] - 3s 478us/step - loss: 0.0681 - acc: 0.9835\n",
      "Epoch 6/10\n",
      "5381/5381 [==============================] - 3s 472us/step - loss: 0.0426 - acc: 0.9918\n",
      "Epoch 7/10\n",
      "5381/5381 [==============================] - 2s 463us/step - loss: 0.0260 - acc: 0.9968\n",
      "Epoch 8/10\n",
      "5381/5381 [==============================] - 3s 492us/step - loss: 0.0165 - acc: 0.9986\n",
      "Epoch 9/10\n",
      "5381/5381 [==============================] - 3s 484us/step - loss: 0.0110 - acc: 0.9993\n",
      "Epoch 10/10\n",
      "5381/5381 [==============================] - 3s 491us/step - loss: 0.0073 - acc: 0.9999\n"
     ]
    }
   ],
   "source": [
    "history = model_3.fit(x=data_train, y=categorical_labels_train, batch_size=50, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8899999933242798"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model_3.evaluate(x_test, categorical_labels_test, verbose=0)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       1.00      0.56      0.71         9\n",
      "        DESC       0.60      0.04      0.08       138\n",
      "        ENTY       0.69      0.70      0.70        94\n",
      "         HUM       0.93      0.82      0.87        65\n",
      "         LOC       0.73      0.89      0.80        81\n",
      "         NUM       0.44      0.91      0.59       113\n",
      "\n",
      "   micro avg       0.61      0.61      0.61       500\n",
      "   macro avg       0.73      0.65      0.63       500\n",
      "weighted avg       0.65      0.61      0.54       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = model_3.predict(x_test)\n",
    "class_predictions = [np.argmax(x) for x in raw_predictions]\n",
    "print(classification_report(y_test, le.inverse_transform(class_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNN multichanell with pre-trained dynamic and static word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = get_cnn_multichannel(embedding_layer_static, embedding_layer_dynamic, max_input_lenght, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5381/5381 [==============================] - 5s 858us/step - loss: 0.2882 - acc: 0.8861\n",
      "Epoch 2/10\n",
      "5381/5381 [==============================] - 4s 731us/step - loss: 0.1074 - acc: 0.9666\n",
      "Epoch 3/10\n",
      "5381/5381 [==============================] - 4s 701us/step - loss: 0.0515 - acc: 0.9874\n",
      "Epoch 4/10\n",
      "5381/5381 [==============================] - 4s 702us/step - loss: 0.0258 - acc: 0.9961\n",
      "Epoch 5/10\n",
      "5381/5381 [==============================] - 4s 812us/step - loss: 0.0135 - acc: 0.9989\n",
      "Epoch 6/10\n",
      "5381/5381 [==============================] - 4s 761us/step - loss: 0.0079 - acc: 0.9996\n",
      "Epoch 7/10\n",
      "5381/5381 [==============================] - 5s 902us/step - loss: 0.0050 - acc: 0.9999\n",
      "Epoch 8/10\n",
      "5381/5381 [==============================] - 5s 841us/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "5381/5381 [==============================] - 4s 787us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "5381/5381 [==============================] - 4s 816us/step - loss: 0.0019 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model_4.fit(x=[data_train, data_train], y=categorical_labels_train, batch_size=50, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9206666488647461"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model_4.evaluate(x=[x_test,x_test], y=categorical_labels_test, verbose=0)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ABBR       1.00      0.67      0.80         9\n",
      "        DESC       0.76      0.12      0.20       138\n",
      "        ENTY       0.39      0.81      0.53        94\n",
      "         HUM       0.89      0.85      0.87        65\n",
      "         LOC       0.66      0.88      0.76        81\n",
      "         NUM       0.93      0.89      0.91       113\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       500\n",
      "   macro avg       0.77      0.70      0.68       500\n",
      "weighted avg       0.73      0.65      0.61       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = model_4.predict([x_test, x_test])\n",
    "class_predictions = [np.argmax(x) for x in raw_predictions]\n",
    "print(classification_report(y_test, le.inverse_transform(class_predictions)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
